{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example for Tokenizer\n",
    "# Reference: https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What is a Tokenizer?\n",
    "    A Tokenizer is one which performs the task of Tokenization. \n",
    "\n",
    "Then what is Tokenization?\n",
    "    It is a process of spliting (or breaking) a string (a stream of text) into \n",
    "        a) words\n",
    "        b) phrases\n",
    "        c) symbols\n",
    "        d) any other meaningful portions(elements/splits)\n",
    "    These splited portions(elements) are called Tokens.\n",
    "\n",
    "What is RegexTokenizer?\n",
    "    It is a tokenizer based on regex (regular expression).\n",
    "    It extracts tokens by\n",
    "        a) using the given regex pattern (default)\n",
    "        b) repeatedly matching the regex\n",
    "        \n",
    "In PySpark, the Tokenizer first converts the text into lower case and then split it by white spaces.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col,udf\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Tokenizer_Example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataset\n",
    "df = spark.createDataFrame([\n",
    "    (1, \"Hello World\"),\n",
    "    (2, \"Welcome PySpark with Examples\"),\n",
    "    (3, \"It's easy, but need regular practice\"),\n",
    "    (4, \"Surely, you will enjoy! Spark runs on\"),\n",
    "    (5, \"Hadoop,Aws,Mesos\")\n",
    "], [\"S_No\", \"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring Tokenizer\n",
    "tokenizer = Tokenizer(inputCol = \"Text\", outputCol = \"Splited_Words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring RegexTokenizer. W indicates white space.\n",
    "r_tokenizer = RegexTokenizer(inputCol = \"Text\", outputCol = \"Splited_words\", pattern=\"\\\\W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count no of tokens\n",
    "count_token = udf(lambda Splited_Words: len(Splited_Words), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+---------------------------------------------+------+\n",
      "|Text                                 |Splited_Words                                |Tokens|\n",
      "+-------------------------------------+---------------------------------------------+------+\n",
      "|Hello World                          |[hello, world]                               |2     |\n",
      "|Welcome PySpark with Examples        |[welcome, pyspark, with, examples]           |4     |\n",
      "|It's easy, but need regular practice |[it's, easy,, but, need, regular, practice]  |6     |\n",
      "|Surely, you will enjoy! Spark runs on|[surely,, you, will, enjoy!, spark, runs, on]|7     |\n",
      "|Hadoop,Aws,Mesos                     |[hadoop,aws,mesos]                           |1     |\n",
      "+-------------------------------------+---------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calling the tokenizer for the given data\n",
    "# Also adding an additional calculated column \"Tokens\"\n",
    "tk = tokenizer.transform(df)\n",
    "tk.select(\"Text\", \"Splited_Words\") \\\n",
    "    .withColumn(\"Tokens\", count_token(col(\"Splited_Words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|                Text|       Splited_Words|Tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|         Hello World|      [hello, world]|     2|\n",
      "|Welcome PySpark w...|[welcome, pyspark...|     4|\n",
      "|It's easy, but ne...|[it, s, easy, but...|     7|\n",
      "|Surely, you will ...|[surely, you, wil...|     7|\n",
      "|    Hadoop,Aws,Mesos|[hadoop, aws, mesos]|     3|\n",
      "+--------------------+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calling the regexTokenizer for the given data. \n",
    "# Also adding an additional calculated column \"Tokens\"\n",
    "# In the above code we have used the truncate=False. what if we do not give it.\n",
    "r_tk = r_tokenizer.transform(df)\n",
    "r_tk.select(\"Text\", \"Splited_Words\")\\\n",
    "    .withColumn(\"Tokens\", count_token(col(\"Splited_Words\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------+-------------------------------------------+------+\n",
      "|Text                                 |Splited_Words                              |Tokens|\n",
      "+-------------------------------------+-------------------------------------------+------+\n",
      "|Hello World                          |[hello, world]                             |2     |\n",
      "|Welcome PySpark with Examples        |[welcome, pyspark, with, examples]         |4     |\n",
      "|It's easy, but need regular practice |[it, s, easy, but, need, regular, practice]|7     |\n",
      "|Surely, you will enjoy! Spark runs on|[surely, you, will, enjoy, spark, runs, on]|7     |\n",
      "|Hadoop,Aws,Mesos                     |[hadoop, aws, mesos]                       |3     |\n",
      "+-------------------------------------+-------------------------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calling the regexTokenizer for the given data. \n",
    "# Also adding an additional calculated column \"Tokens\"\n",
    "# with truncate=False\n",
    "r_tk = r_tokenizer.transform(df)\n",
    "r_tk.select(\"Text\", \"Splited_Words\")\\\n",
    "    .withColumn(\"Tokens\", count_token(col(\"Splited_Words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" regex tokenizer considers even apostrophe. \n",
    "It also considers a string (a stream of text) without space but with.\n",
    "The normal tokenizer does not do this.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
